version: "3.7"
services:  
    postgres:
        image: data-etl-postgres
        volumes:
            - ./docker/docker-postgres/pg-init-scripts:/docker-entrypoint-initdb.d
        environment:
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
        ports:
            - "5432:5432"

    airflow-web:
        image: data-etl-airflow
        restart: on-failure
        depends_on:
            - postgres
        environment:
            - LOAD_EX=n
            - EXECUTOR=Local
        logging:
            options:
                max-size: 10m
                max-file: "3"
        volumes:
            - ./dags:/usr/local/airflow/dags
            - ./spark/app:/home/workspace/app
            - ./spark/jars:/home/workspace/jars
            - ./data:/home/workspace/data
        ports:
            - "8282:8080"
        command: webserver
        healthcheck:
            test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
            interval: 30s
            timeout: 30s
            retries: 3
            
    # Spark with 3 workers
    spark:
        image: data-etl-spark-master
        user: root
        environment:
            - SPARK_MODE=master
        volumes:
            - ./spark/app:/home/workspace/app
            - ./spark/jars:/home/workspace/jars
            - ./data:/home/workspace/data
        ports:
            - "8080:8080"
            - "7077:7077"

    spark-worker-1:
        image: data-etl-spark-worker
        depends_on:
            - spark
        environment:
            - SPARK_MASTER_URL=spark://spark:7077
            - SPARK_WORKER_MEMORY=1G
            - SPARK_WORKER_CORES=1
        ports:
            - "8081:8081"
        volumes:
            - ./spark/app:/home/workspace/app
            - ./spark/jars:/home/workspace/jars
            - ./spark/conf/spark-env.sh:/usr/bin/spark-3.2.1-bin-hadoop3.2/conf/spark-env.sh

    spark-worker-2:
        image: data-etl-spark-worker
        depends_on:
            - spark
        environment:
            - SPARK_MASTER_URL=spark://spark:7077
            - SPARK_WORKER_MEMORY=1G
            - SPARK_WORKER_CORES=1
        ports:
            - "8082:8081"
        volumes:
            - ./spark/app:/home/workspace/app
            - ./spark/jars:/home/workspace/jars
            - ./spark/conf/spark-env.sh:/usr/bin/spark-3.2.1-bin-hadoop3.2/conf/spark-env.sh

    spark-worker-3:
        image: data-etl-spark-worker
        depends_on:
            - spark
        environment:
            - SPARK_MASTER_URL=spark://spark:7077
            - SPARK_WORKER_MEMORY=1G
            - SPARK_WORKER_CORES=1
        ports:
            - "8083:8081"
        volumes:
            - ./spark/app:/home/workspace/app
            - ./spark/jars:/home/workspace/jars
            - ./spark/conf/spark-env.sh:/usr/bin/spark-3.2.1-bin-hadoop3.2/conf/spark-env.sh

    jupyterlab:
        image: data-etl-notebook
        ports:
            - 8888:8888
            - 4040:4040
        volumes:
            - ./notebooks:/home/workspace/notebooks
            - ./spark:/home/workspace/spark
            - ./data:/home/workspace/data