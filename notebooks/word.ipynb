{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57ed6c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import pandas\n",
    "import jieba\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3770275c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.set_dictionary(\"/home/workspace/data/dict_tw.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abc700fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/18 09:33:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master(\"spark://spark:7077\")\n",
    "         .config(\"spark.executor.memory\", \"512m\")\n",
    "         .getOrCreate()\n",
    "         )\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd496b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"/home/workspace/data/input/input.json\")\n",
    " \n",
    "# returns JSON object as\n",
    "# a dictionary\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33758ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f264982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "newJson = str(data)\n",
    "df = spark.read.json(sc.parallelize([newJson]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "175e49d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed2cb616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- content: string (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb613a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c8def12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def regular(w):\n",
    "    line = re.findall('[\\u4e00-\\u9fa5]+', w)\n",
    "    if len(line) > 0:\n",
    "        return line\n",
    "\n",
    "\n",
    "def get_cut(content):\n",
    "    seg_list = jieba.cut(content, cut_all=True, HMM=True)\n",
    "    return seg_list\n",
    "\n",
    "\n",
    "def text_cleaning(paragraph):\n",
    "    merge_words = []\n",
    "    words = get_cut(paragraph)\n",
    "    for w in words:\n",
    "        if len(w) > 1:\n",
    "            w = regular(w)\n",
    "            if w is not None:\n",
    "                merge_words.extend(w)\n",
    "    return \"\\n\".join(merge_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5dd1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91fb7360",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "titleDf = df.select(\n",
    "    col(\"link_id\"),\n",
    "    col(\"title\")\n",
    ")\n",
    "wordDf = df.select(\n",
    "    col(\"content\"),\n",
    "    col(\"link_id\")\n",
    ").rdd.map(\n",
    "    lambda x: (x[0], x[1])\n",
    ").flatMap(\n",
    "    lambda x: ([(w, x[1]) for w in text_cleaning(x[0]).split(\"\\n\")])\n",
    ").map(\n",
    "    lambda x: (x, 1)\n",
    ").reduceByKey(\n",
    "    lambda x, y: x + y\n",
    ").map(\n",
    "    lambda x: (x[0][0], x[0][1], x[1])\n",
    ").toDF([\"word\", \"index\", \"count\"])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "516bcb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# titleDf.show(truncate = False)\n",
    "# wordDf.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e031eeeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1fcf05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
